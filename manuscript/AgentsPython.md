# Using Microsoft’s Autogen for Python Tool Calling Agents

Microsoft’s Autogen agent framework is excellent but it was designed to work with OpenAI’s models. Here we use Ollama with the **qwen2.5:14b** and a slightly modified version of a [Microsoft example using Python and Matplotlib with Autogen](https://microsoft.github.io/autogen/0.4.0.dev3//user-guide/core-user-guide/quickstart.html) and the [Microsoft Autogen with Ollama documentation](https://microsoft.github.io/autogen/0.2/docs/topics/non-openai-models/local-ollama/) is a useful reference.

## Example Implementation

I experimented with several local models using Ollama with mediocre results but the larger **qwen2.5:14b** model works very well. If you are running on a Mac you will need an Apple Silicon chip with 16G of memory to run this model.

```python
from autogen import AssistantAgent, UserProxyAgent

# Requirements:
# pip install autogen ollama fix_busted_json yfinance matplotlib

config_list = [
 {
    "model": "qwen2.5:14b",   # Choose a model that supports tool calling
    "api_type": "ollama",     # Specify Ollama as the API type
    "client_host": "http://localhost:11434",  # local Ollama server
    "api_key": "fakekey",
    "native_tool_calls": True # Enable native tool calling 
 }
]

# Create the AssistantAgent using the local model config
assistant = AssistantAgent("assistant",
                           llm_config={"config_list": config_list})

# Create the UserProxyAgent; adjust code_execution_config as needed.
user_proxy = UserProxyAgent(
    "user_proxy",
    code_execution_config={"work_dir": "coding", "use_docker": False}
)

# Initiate an automated chat between the agents.
user_proxy.initiate_chat(
    assistant,
    message="Plot a chart of NVDA and TESLA stock price change YTD."
)
```

This code sets up a multi-agent workflow using AutoGen by configuring an assistant agent and a user proxy agent. First, it defines a configuration for a local Ollama-based model (here, "qwen2.5:14b") that supports native tool calling. The configuration specifies details such as the API type ("ollama"), the local server URL ("http://localhost:11434"), a placeholder API key, and an option to enable native tool calls. This configuration is then passed to instantiate the AssistantAgent, which uses it to generate responses based on the local LLM.

This example uses a UserProxyAgent configured for Python code execution with a designated working directory ("coding") and Docker disabled. Finally, the user proxy agent initiates an automated chat with the assistant by sending a message requesting the plotting of a chart for NVDA and TESLA stock price changes year-to-date. This setup demonstrates a simple, automated multi-agent interaction where the assistant generates responses (potentially including code) and the user proxy executes those responses to fulfill the requested task. You will either want to remove the generated code directory **coding** after running this example of add **coding** to your **.gitignore** file.

## Example Output

![Plot generated by Matplotlib run by agent](images/autogen1.jpg)

Here is a partial listing of the output:

```text
$ python autogen_python_example.py  

user_proxy (to assistant):

Plot a chart of NVDA and TESLA stock price change YTD.

-------------------------------------------------------------
assistant (to user_proxy):

To plot a chart of NVDA (NVIDIA) and Tesla's stock price changes year-to-date, we will need to fetch their historical data from an API or a financial data source such as Yahoo Finance.

Let's use Python with the `yfinance` library to get the data and then plot it using `matplotlib`.

Here is a step-by-step plan:

1. **Install Required Libraries**: If not already installed, install `yfinance` for fetching stock data and `pandas` & `matplotlib` for processing and plotting the data.
2. **Fetch Stock Data**: Use `yfinance` to get the year-to-date historical data for NVDA and TSLA stocks.
3. **Plot Data**: Plot the closing prices of both stocks on a chart.

Let's start by fetching and plotting the stock price data with Python code:

``python
# filename: plot_stock_prices.py

import yfinance as yf
import matplotlib.pyplot as plt
import pandas as pd

# Fetch year-to-date historical data for NVDA and TSLA
stocks = ['NVDA', 'TSLA']
data = yf.download(stocks, start='2023-01-01')['Close']

# Plot the closing prices of both stocks
plt.figure(figsize=(14,7))
for stock in stocks:
    plt.plot(data.index, data[stock], label=stock)
    
plt.title('Year-to-date Stock Prices for NVDA and TSLA')
plt.xlabel('Date')
plt.ylabel('Stock Price (USD)')
plt.legend()
plt.grid(True)
plt.show()
``

You can execute the above Python script to generate the plot. Please make sure you have `yfinance`, `matplotlib`, and `pandas` installed in your environment. If not, install them by running:

``sh
pip install yfinance matplotlib pandas
``

After executing the code, you should see a chart showing the year-to-date closing prices for both NVDA and TSLA stocks.

Once you have executed this script, please inform me of any issues or if everything is plotted correctly.

-------------------------------------------------------------
Replying as user_proxy. Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:

>>>>>>>> NO HUMAN INPUT RECEIVED.
>>>>>>>> USING AUTO REPLY...
>>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...
```

## Wrap Up for Using Microsoft’s Autogen for Python Tool Calling Agents

Here we experimented with using Ollama and a local model to use Microsoft’s powerful Autogen agent framework.

Microsoft’s Autogen framework for Python tool calling agents streamlines the development of multi-agent applications by enabling seamless interaction between an assistant agent and a user proxy agent. In the example, we configured a local LLM (using a model like “qwen2.5:14b” via Ollama) and instantiated an AssistantAgent to generate responses and code, while a UserProxyAgent was set up to execute the generated code. This automated conversation allowed the agents to collaborate on a task—plotting a chart for NVDA and TESLA stock price changes year-to-date—demonstrating how Autogen bridges code generation and execution in an autonomous workflow.   ￼

This example highlights Autogen’s ability to leverage native tool calling within a Python environment, reducing the need for manual intervention during code execution and debugging. By decoupling the generation of task-specific code from its execution, developers can build systems that are both flexible and scalable. The assistant agent focuses on planning and generating code, while the user proxy agent reliably executes that code, creating an effective feedback loop for refining results. This pattern not only simplifies complex workflows but also provides a foundation for robust, error-resilient applications.

To further explore the potential of Autogen, you dear reader might experiment with integrating additional tools—such as web scraping modules, database connectors, or advanced visualization libraries—to expand the capabilities of their agents. Another interesting avenue is to adjust the configuration parameters: try different LLM models, enable Docker-based code execution, or incorporate human-in-the-loop feedback to refine responses. Additionally, extending the workflow to include more specialized agents (e.g., a dedicated CodeExecutorAgent or a DebuggingAgent) can provide insights into multi-agent orchestration and the scalability of autonomous systems. These experiments will help readers understand the full versatility of Autogen in creating dynamic, multi-step applications.
