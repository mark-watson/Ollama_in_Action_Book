# Ollama REST APIs

The Ollama REST API provides a straightforward way to integrate local LLM capabilities into applications, offering both Ollama-specific endpoints and OpenAI-compatible interfaces. The native Ollama API is accessible at http://localhost:11434/api and supports core operations like /generate for text generation, /chat for conversational interactions, and /embeddings for vector representations. Basic usage involves sending POST requests with JSON payloads containing the model name and prompt. For example, generating text is as simple as sending a POST to /api/generate with {"model": "llama3.2:latest", "prompt": "Your text here"}.

Ollama's OpenAI-compatible API interface allows developers to use their existing OpenAI-based code with minimal modifications. By pointing your OpenAI client library to http://localhost:11434/v1 instead of OpenAI's endpoints, you can often use the same code structure and request formats. The compatible endpoints include /chat/completions, /completions, and /embeddings, matching OpenAI's API structure. This compatibility layer is particularly valuable for teams transitioning from cloud-based to local LLM deployments, as it reduces the refactoring effort significantly.

Advanced features of the Ollama API include streaming responses, which can be enabled by setting stream: true in the request body, and model management endpoints like /pull for downloading models and /list for viewing available models. The API also supports multimodal interactions through the native endpoints, allowing for image analysis with compatible models like llava. Error handling follows standard HTTP status codes, with detailed error messages in the response body to aid debugging. Rate limiting and concurrent request handling are managed automatically by the Ollama server, though these can be configured through server settings.

When working with the OpenAI compatibility layer, it's important to note some differences in parameter naming and available options. While core functionality matches, some OpenAI-specific features like function calling might require adaptation to work with Ollama's implementation. Authentication is typically not required for local deployments, though you can add basic authentication through a reverse proxy if needed. For production deployments, consider using environment variables to switch between OpenAI and Ollama endpoints, allowing for flexible configuration across development and production environments. The compatibility layer also supports streaming responses through server-sent events, maintaining consistency with OpenAI's implementation.
