# Examples Using the Model Context Protocol(MCP)

WORK IN PROGRESS - NOT IN BOOK

We have seen examples of tool use with Ollama. Here we broaden this discusion to describing and using the Model Context Protocol(MCP). We will use two simple MCP series: Search summaries using DuckDuckGo and and a custom research service for events in Flagstaff Arizona.

## The Context Problem (Optional Background Material)

The increasing sophistication of Large Language Models (LLMs) has transformed their role from simple interactive chatbots into active agents capable of complex decision-making. However, this progress is fundamentally constrained by two factors: the static nature of their training data and their limited, often unreliable, capacity to interact with the dynamic, real world. Prior to standardization efforts, connecting an LLM to external operational systems, such as enterprise databases, Customer Relationship Management (CRM) tools, or specialized application programming interfaces (APIs), required developers to write custom, time-intensive API wrappers for every unique integration. This bespoke approach led to a fragmented, inconsistent, and highly difficult-to-maintain ecosystem. The MCP was introduced to solve this fragmentation, providing an open-source standard for connecting AI applications to external data sources, tools, and workflows. MCP standardizes how applications can share contextual information and expose tools and capabilities to AI systems, taking architectural inspiration from standards like the Language Server Protocol (LSP). By defining a universal language for AI models to interact with the outside world, MCP functions like the "USB-C port for AI," replacing dozens of custom connections with a single, universal adapter, thereby simplifying development and facilitating a plug-and-play ecosystem. Crucially, this protocol achieves a critical separation of concerns by isolating the execution of external actions (the MCP Server's responsibility) from the core reasoning and interaction capabilities of the LLM (the Client/Host).

The standardization provided by MCP must be paired with a robust and controllable execution environment for the LLM itself. This is where Ollama becomes the foundational component, simplifying the deployment and management of a wide array of high-performing, open-source models—such as Llama, Mistral, and Gemma on local or commodity hardware. Ollama abstracts the low-level complexities of managing these models, relying on optimized libraries like llama.cpp to ensure efficient local inference. This local deployment capability is paramount, offering significant advantages in data sovereignty, cost efficiency, and full control over the specific models and versions utilized. By keeping sensitive data entirely within the user's infrastructure, Ollama addresses critical privacy and security concerns that prevent the adoption of cloud-based LLMs in regulated industries. Furthermore, Ollama is engineered to support advanced agentic requirements essential for MCP integration, including native support for Tool Calling and the enforcement of Structured Outputs using JSON schemas. This ability to constrain the model’s output ensures that when the LLM decides to use an external tool, the resulting request is machine-readable and precisely formatted, guaranteeing that the subsequent MCP client can reliably interpret and execute the intended action. The presence of this capability positions Ollama as the intelligent decision engine that validates the request, confirming the LLM is an active, structured component in the agent workflow, translating natural language intent into a structured protocol command.   

The strategic power we explore in this chapter lies in the synergy created by pairing the Model Context Protocol with the local execution capabilities of Ollama. MCP provides the standardized pathway for external action, while Ollama furnishes the secure, governed reasoning engine. This combined architecture transforms the LLM from a passive generator into an intelligent, multi-step agent. The workflow begins when the Ollama-powered LLM, analyzing the user's natural language request, determines that an external capability is needed; the model then generates a structured response indicating the required tool and its parameters. This structured command is then handed off to the MCP Client, which communicates with the appropriate MCP Server to execute the function—whether that involves performing a web search or querying a proprietary database. Crucially, the server passes the tool's output back to the client, which injects this real-time, verified information back into the conversation context before the final prompt is sent to Ollama. By consistently accessing accurate, external data via this standardized method, the LLM is reliably grounded in real-time facts, fundamentally mitigating the problem of model hallucination and ensuring the final response is trustworthy and up-to-date. This stack establishes the architectural template for secure, proprietary agent development, meeting high-demand enterprise requirements where control and customization—specifically utilizing specialized, fine-tuned models and guaranteeing local data access—are paramount.   

## MCP Standardization

To grasp the practical examples of the DuckDuckGo and Flagstaff servers, a deeper understanding of the core MCP architecture is required. The protocol relies on three primary components working in concert: the MCP Host, the MCP Client, and the MCP Server.14 The Host represents the user-facing AI application, such as an integrated development environment (IDE) or a custom application, which orchestrates the entire workflow and manages the user experience.14 The Host instantiates an MCP Client, a dedicated connection component responsible for managing the one-to-one link to an external service and interpreting the protocol messages.14The essential building block is the MCP Server, which acts as the crucial bridge between the standardized protocol layer and non-standard external systems like REST APIs or databases.12 MCP servers define their capabilities through a set of self-describing structures 6: Tools are functions designed to execute code or produce a side effect in the real world, analogous to POST endpoints. Both examples in this chapter utilize Tools.6 Resources are used to load information into the LLM's context without side effects, similar to GET endpoints.6 Prompts define reusable interaction patterns or templates to guide the LLM's behavior. Communication within this ecosystem adheres strictly to the JSON-RPC 2.0 message format.12The standardization of capabilities ensures that the integration cost is minimized. If a developer builds a single, compliant MCP server for a system—for example, a CRM or a file system—any LLM application that supports MCP can instantly use it.1 The protocol is also flexible in deployment, supporting standard transport mechanisms, including Standard Input/Output (stdio), Server-Sent Events (SSE), and Streamable HTTP.6 This flexibility allows for deployment on systems ranging from lightweight local scripts to robust, scalable web services.The architectural roles of these components in the Ollama integration loop are summarized below

MCP Architectural Components and RolesComponentPrimary 

